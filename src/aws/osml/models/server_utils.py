#  Copyright 2023-2026 Amazon.com, Inc. or its affiliates.

import logging
import os
import random
import sys
import time
from secrets import token_hex
from typing import Dict, List, Optional, Union
from urllib.parse import unquote

import json_logging
from flask import Flask, request
from osgeo import gdal

# Enable exceptions for GDAL
gdal.UseExceptions()


def build_logger(level: int = logging.WARN) -> logging.Logger:
    """
    Utility function to create and configure a logger that outputs logs in JSON format.

    :param level: Logging level (default: logging.WARN).
    :return: Configured logger instance.
    """

    # Create a logger at the given level
    logger = logging.getLogger(__name__)

    # Ensure no duplicate handlers
    if not logger.hasHandlers():
        # Create a handler that writes to sys.stdout
        handler = logging.StreamHandler(sys.stdout)

        # Add the handler to the logger
        logger.addHandler(handler)
        logging.root.addHandler(handler)

    logger.setLevel(level)
    return logger


def setup_server(app: Flask):
    """
    The assumption is that this script will be the ENTRYPOINT for the inference
    container. SageMaker will launch the container with the "serve" argument. We
    also have the option of using multiple models from this single container;
    only one model will be active at a time (i.e., this is not a Multi Model Server),
    so it can be selected by name using the "model" parameter.

    :param app: The flask application to set up
    :return: None
    """
    port = int(os.getenv("SAGEMAKER_BIND_TO_PORT", 8080))

    # Configure Waitress thread count for better concurrency
    # Default to 8 threads for GPU instances, 4 for CPU
    # Can be overridden via WAITRESS_THREADS environment variable
    default_threads = 16  # Good for GPU inference with concurrent requests
    threads = int(os.getenv("WAITRESS_THREADS", default_threads))

    # Channel timeout (seconds) - how long to wait for a request to complete
    channel_timeout = int(os.getenv("WAITRESS_CHANNEL_TIMEOUT", 120))

    # Connection limit - max concurrent connections
    connection_limit = int(os.getenv("WAITRESS_CONNECTION_LIMIT", 100))

    # Log all arguments in a single log message
    app.logger.info(f"Initializing OSML Model Flask server on port {port}!")
    app.logger.info(
        f"Waitress configuration: threads={threads}, channel_timeout={channel_timeout}s, connection_limit={connection_limit}"
    )
    app.logger.info(
        "Note: 'Task queue depth' warnings indicate request backlog. Increase WAITRESS_THREADS if these persist."
    )

    # Start the simple web application server using Waitress.
    # Flask's app.run() is only intended to be used in development
    #  mode, so this provides a solution for hosting the application.
    from waitress import serve

    serve(
        app,
        host="0.0.0.0",
        port=port,
        threads=threads,
        channel_timeout=channel_timeout,
        connection_limit=connection_limit,
        clear_untrusted_proxy_headers=True,
    )


def build_flask_app(logger: logging.Logger) -> Flask:
    """
    Create a Flask app and configure it to use the provided logger.
    The logger will output logs in JSON format and write to sys.stdout.

    :param logger: The logger to use with the application
    :return: Configured Flask app instance.
    """
    # Create a Flask app instance
    app = Flask(__name__)

    # Clear default Flask log handlers
    app.logger.handlers.clear()

    # Add the provided logger's handlers to the Flask app logger
    for handler in logger.handlers:
        app.logger.addHandler(handler)

    # Ensure the Flask app logger uses the same logging level as the custom logger
    app.logger.setLevel(logger.level)

    if json_logging._current_framework is None:
        json_logging.init_flask(enable_json=True)

    return app


def detect_to_feature(
    fixed_object_bbox: List[float],
    fixed_object_mask: Optional[List[List[float]]] = None,
    detection_score: Optional[float] = 1.0,
    detection_type: Optional[str] = "sample_object",
) -> Dict[str, Union[str, list]]:
    """
    Converts the bbox object into a sample GeoJSON formatted detection.
    The model container does not normally provide the world coordinates,
    so they're defaulted to 0,0 here since GeoJSON features require a geometry.

    :param detection_type: Class assigned to the detection
    :param detection_score: Confidence score assigned to the detection
    :param fixed_object_mask: Polygon version of mask generated by the model
    :param fixed_object_bbox: Bounding box to transform into a geojson feature
    :return: dict: Dictionary representation of a geojson feature
    """

    feature = {
        "type": "Feature",
        "geometry": None,
        "id": token_hex(16),
        "properties": {
            "imageGeometry": {"type": "Point", "coordinates": [0.0, 0.0]},
            "imageBBox": fixed_object_bbox,
            "featureClasses": [{"iri": detection_type, "score": detection_score}],
            "modelMetadata": {"modelName": "centerpoint", "ontologyName": "centerpoint", "ontologyVersion": "1.0.0"},
            "image_id": token_hex(16),
        },
    }

    if fixed_object_mask is not None:
        feature["properties"]["imageGeometry"] = {"type": "Polygon", "coordinates": [fixed_object_mask]}

    return feature


def parse_custom_attributes() -> Dict[str, str]:
    """
    Parse the SageMaker CustomAttributes header from the request.

    The CustomAttributes header contains comma-separated key=value pairs that can
    be used to pass additional information to the model endpoint.

    Example header value: "mock_latency_mean=500,mock_latency_std=50,trace_id=abc123,text_prompt=cars"

    Values are URL-decoded to handle spaces and special characters properly.
    For example, "text_prompt=sport%20cars" will be decoded to "sport cars".

    :return: Dictionary of parsed key-value pairs from the CustomAttributes header.
             Returns an empty dictionary if the header is not present or parsing fails.
    """
    # Get the CustomAttributes header from the request
    custom_attributes = request.headers.get("X-Amzn-SageMaker-Custom-Attributes", "")

    if not custom_attributes:
        return {}

    # Parse the custom attributes
    attributes = {}
    try:
        for pair in custom_attributes.split(","):
            if "=" in pair:
                key, value = pair.split("=", 1)
                # URL-decode the value to handle spaces and special characters
                # e.g., "detect%20cars" becomes "detect cars"
                decoded_value = unquote(value.strip())
                attributes[key.strip()] = decoded_value
    except Exception:
        # If parsing fails, return empty dictionary
        return {}

    return attributes


def simulate_model_latency() -> None:
    """
    Simulate model inference latency by sleeping for a random duration based on
    custom attributes provided in the request header.

    This function checks for 'mock_latency_mean' and 'mock_latency_std' parameters
    in the CustomAttributes header. If found, it sleeps for a random number of
    milliseconds drawn from a normal distribution with the specified mean and
    standard deviation.

    If only mock_latency_mean is provided, mock_latency_std defaults to 10% of the mean.
    If neither parameter is present, no sleep occurs.

    :return: None
    """
    # Get parsed custom attributes
    attributes = parse_custom_attributes()

    # Check for mock_latency_mean
    if "mock_latency_mean" not in attributes:
        return

    try:
        # Parse the mean latency
        mean_ms = float(attributes["mock_latency_mean"])

        # Parse or calculate the standard deviation
        if "mock_latency_std" in attributes:
            std_ms = float(attributes["mock_latency_std"])
        else:
            # Default to 10% of mean if std is not provided
            std_ms = mean_ms * 0.1

        # Generate a random latency using normal distribution
        latency_ms = random.gauss(mean_ms, std_ms)

        # Ensure latency is non-negative
        latency_ms = max(0, latency_ms)

        # Convert to seconds and sleep
        time.sleep(latency_ms / 1000.0)

    except (ValueError, TypeError):
        # If conversion to float fails, just return without sleeping
        return
